{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import PReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import add\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1021, 1344, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_image('../img/bug/b_bigbug0000_croppped.png').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data is too large to fit in memory, we will use a generator to load the data in batches.\n",
    "def load_image(path):\n",
    "    try:\n",
    "        img = cv2.imread(path)\n",
    "        # If the image has not 3 channels, generate a 3 channels image from the gray scale image\n",
    "        if img.shape[2]!=3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        return img\n",
    "    except:\n",
    "        print(path)\n",
    "        return None\n",
    "\n",
    "def load_data(batch_images, data_augmentation=0):\n",
    "    imgs_hr, imgs_lr = [], []\n",
    "    for img_path in batch_images:\n",
    "        img_hr = load_image(img_path)\n",
    "        # If the image is not at 1920x1080, resize it to 1920x1080\n",
    "        if img_hr.shape[0]!=540 or img_hr.shape[1]!=960:\n",
    "            img_hr = cv2.resize(img_hr, (540, 960), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Make data augmentation here\n",
    "        if data_augmentation>0:\n",
    "            img_hr = aug_image(img_hr, data_augmentation)\n",
    "        img_lr = cv2.resize(img_hr, (int(img_hr.shape[0]/4), int(img_hr.shape[1]/4)), interpolation=cv2.INTER_CUBIC)\n",
    "        imgs_hr.append(img_hr)\n",
    "        imgs_lr.append(img_lr)\n",
    "    imgs_hr = np.array(imgs_hr)\n",
    "    imgs_lr = np.array(imgs_lr)\n",
    "    return imgs_hr, imgs_lr\n",
    "\n",
    "# Augment the image by applying random rotation, random zoom and random translation\n",
    "def aug_image(img,num_of_aug):\n",
    "    img_list = []\n",
    "    for i in range(num_of_aug):\n",
    "        # Random rotation\n",
    "        angle = np.random.randint(0,360)\n",
    "        img_rot = rotate_image(img, angle)\n",
    "        # Random zoom\n",
    "        zoom_factor = np.random.randint(1,5)\n",
    "        img_zoom = zoom_image(img_rot, zoom_factor)\n",
    "        # Random translation\n",
    "        x_shift = np.random.randint(-50,50)\n",
    "        y_shift = np.random.randint(-50,50)\n",
    "        img_shift = shift_image(img_zoom, x_shift, y_shift)\n",
    "        img_list.append(img_shift)\n",
    "    return img_list[np.random.randint(0,num_of_aug)]\n",
    "\n",
    "\n",
    "def rotate_image(img, angle):\n",
    "    rows,cols = img.shape[0:2]\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),angle,1)\n",
    "    img_rot = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return img_rot\n",
    "\n",
    "def zoom_image(img, zoom_factor):\n",
    "    rows,cols = img.shape[0:2]\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),0,zoom_factor)\n",
    "    img_zoom = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return img_zoom\n",
    "\n",
    "def shift_image(img, x_shift, y_shift):\n",
    "    rows,cols = img.shape[0:2]\n",
    "    M = np.float32([[1,0,x_shift],[0,1,y_shift]])\n",
    "    img_shift = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return img_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Activation, Dense, Conv2D, BatchNormalization, \\\n",
    "    LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class Discriminator:\n",
    "    \"\"\"\n",
    "    Implementation of the discriminator network for the adversarial\n",
    "    component of the perceptual loss.\n",
    "\n",
    "    Args:\n",
    "        patch_size: integer, determines input size as (patch_size, patch_size, 3).\n",
    "        kernel_size: size of the kernel in the conv blocks.\n",
    "\n",
    "    Attributes:\n",
    "        model: Keras model.\n",
    "        name: name used to identify what discriminator is used during GANs training.\n",
    "        model._name: identifies this network as the discriminator network\n",
    "            in the compound model built by the trainer class.\n",
    "        block_param: dictionary, determines the number of filters and the strides for each\n",
    "            conv block.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patch_heigt,patch_width, kernel_size=3):\n",
    "        self.patch_heigt = patch_heigt\n",
    "        self.patch_width = patch_width\n",
    "        self.kernel_size = kernel_size\n",
    "        self.block_param = {}\n",
    "        self.block_param['filters'] = (64, 128, 128, 256, 256, 512, 512)\n",
    "        self.block_param['strides'] = (2, 1, 2, 1, 1, 1, 1)\n",
    "        self.block_num = len(self.block_param['filters'])\n",
    "        self.model = self._build_disciminator()\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.model._name = 'discriminator'\n",
    "        self.name = 'srgan-large'\n",
    "    \n",
    "    def _conv_block(self, input, filters, strides, batch_norm=True, count=None):\n",
    "        \"\"\" Convolutional layer + Leaky ReLU + conditional BN. \"\"\"\n",
    "        \n",
    "        x = Conv2D(\n",
    "            filters,\n",
    "            kernel_size=self.kernel_size,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            name='Conv_{}'.format(count),\n",
    "        )(input)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization(momentum=0.8)(x)\n",
    "        return x\n",
    "    \n",
    "    def _build_disciminator(self):\n",
    "        \"\"\" Puts the discriminator's layers together. \"\"\"\n",
    "        \n",
    "        HR = Input(shape=(self.patch_heigt, self.patch_width, 3))\n",
    "        x = self._conv_block(HR, filters=64, strides=1, batch_norm=False, count=1)\n",
    "        for i in range(self.block_num):\n",
    "            x = self._conv_block(\n",
    "                x,\n",
    "                filters=self.block_param['filters'][i],\n",
    "                strides=self.block_param['strides'][i],\n",
    "                count=i + 2,\n",
    "            )\n",
    "        x = Dense(self.block_param['filters'][-1] * 2, name='Dense_1024')(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        # x = Flatten()(x)\n",
    "        x = Dense(1, name='Dense_last')(x)\n",
    "        HR_v_SR = Activation('sigmoid')(x)\n",
    "        \n",
    "        discriminator = Model(inputs=HR, outputs=HR_v_SR)\n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_array(image_array, expand=True):\n",
    "    \"\"\" Process a 3-dimensional array into a scaled, 4 dimensional batch of size 1. \"\"\"\n",
    "    \n",
    "    image_batch = image_array / 255.0\n",
    "    if expand:\n",
    "        image_batch = np.expand_dims(image_batch, axis=0)\n",
    "    return image_batch\n",
    "\n",
    "\n",
    "def process_output(output_tensor):\n",
    "    \"\"\" Transforms the 4-dimensional output tensor into a suitable image format. \"\"\"\n",
    "    \n",
    "    sr_img = output_tensor.clip(0, 1) * 255\n",
    "    sr_img = np.uint8(sr_img)\n",
    "    return sr_img\n",
    "\n",
    "\n",
    "def split_image_into_overlapping_patches(image_array, patch_size, padding_size=2):\n",
    "    \"\"\" Splits the image into partially overlapping patches.\n",
    "\n",
    "    The patches overlap by padding_size pixels.\n",
    "\n",
    "    Pads the image twice:\n",
    "        - first to have a size multiple of the patch size,\n",
    "        - then to have equal padding at the borders.\n",
    "\n",
    "    Args:\n",
    "        image_array: numpy array of the input image.\n",
    "        patch_size: size of the patches from the original image (without padding).\n",
    "        padding_size: size of the overlapping area.\n",
    "    \"\"\"\n",
    "    \n",
    "    xmax, ymax, _ = image_array.shape\n",
    "    x_remainder = xmax % patch_size\n",
    "    y_remainder = ymax % patch_size\n",
    "    \n",
    "    # modulo here is to avoid extending of patch_size instead of 0\n",
    "    x_extend = (patch_size - x_remainder) % patch_size\n",
    "    y_extend = (patch_size - y_remainder) % patch_size\n",
    "    \n",
    "    # make sure the image is divisible into regular patches\n",
    "    extended_image = np.pad(image_array, ((0, x_extend), (0, y_extend), (0, 0)), 'edge')\n",
    "    \n",
    "    # add padding around the image to simplify computations\n",
    "    padded_image = pad_patch(extended_image, padding_size, channel_last=True)\n",
    "    \n",
    "    xmax, ymax, _ = padded_image.shape\n",
    "    patches = []\n",
    "    \n",
    "    x_lefts = range(padding_size, xmax - padding_size, patch_size)\n",
    "    y_tops = range(padding_size, ymax - padding_size, patch_size)\n",
    "    \n",
    "    for x in x_lefts:\n",
    "        for y in y_tops:\n",
    "            x_left = x - padding_size\n",
    "            y_top = y - padding_size\n",
    "            x_right = x + patch_size + padding_size\n",
    "            y_bottom = y + patch_size + padding_size\n",
    "            patch = padded_image[x_left:x_right, y_top:y_bottom, :]\n",
    "            patches.append(patch)\n",
    "    \n",
    "    return np.array(patches), padded_image.shape\n",
    "\n",
    "\n",
    "def stich_together(patches, padded_image_shape, target_shape, padding_size=4):\n",
    "    \"\"\" Reconstruct the image from overlapping patches.\n",
    "\n",
    "    After scaling, shapes and padding should be scaled too.\n",
    "\n",
    "    Args:\n",
    "        patches: patches obtained with split_image_into_overlapping_patches\n",
    "        padded_image_shape: shape of the padded image contructed in split_image_into_overlapping_patches\n",
    "        target_shape: shape of the final image\n",
    "        padding_size: size of the overlapping area.\n",
    "    \"\"\"\n",
    "    \n",
    "    xmax, ymax, _ = padded_image_shape\n",
    "    patches = unpad_patches(patches, padding_size)\n",
    "    patch_size = patches.shape[1]\n",
    "    n_patches_per_row = ymax // patch_size\n",
    "    \n",
    "    complete_image = np.zeros((xmax, ymax, 3))\n",
    "    \n",
    "    row = -1\n",
    "    col = 0\n",
    "    for i in range(len(patches)):\n",
    "        if i % n_patches_per_row == 0:\n",
    "            row += 1\n",
    "            col = 0\n",
    "        complete_image[\n",
    "        row * patch_size: (row + 1) * patch_size, col * patch_size: (col + 1) * patch_size, :\n",
    "        ] = patches[i]\n",
    "        col += 1\n",
    "    return complete_image[0: target_shape[0], 0: target_shape[1], :]\n",
    "\n",
    "\n",
    "class ImageModel:\n",
    "    \"\"\"ISR models parent class.\n",
    "\n",
    "    Contains functions that are common across the super-scaling models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict(self, input_image_array, by_patch_of_size=None, batch_size=10, padding_size=2):\n",
    "        \"\"\"\n",
    "        Processes the image array into a suitable format\n",
    "        and transforms the network output in a suitable image format.\n",
    "\n",
    "        Args:\n",
    "            input_image_array: input image array.\n",
    "            by_patch_of_size: for large image inference. Splits the image into\n",
    "                patches of the given size.\n",
    "            padding_size: for large image inference. Padding between the patches.\n",
    "                Increase the value if there is seamlines.\n",
    "            batch_size: for large image inferce. Number of patches processed at a time.\n",
    "                Keep low and increase by_patch_of_size instead.\n",
    "        Returns:\n",
    "            sr_img: image output.\n",
    "        \"\"\"\n",
    "        \n",
    "        if by_patch_of_size:\n",
    "            lr_img = process_array(input_image_array, expand=False)\n",
    "            patches, p_shape = split_image_into_overlapping_patches(\n",
    "                lr_img, patch_size=by_patch_of_size, padding_size=padding_size\n",
    "            )\n",
    "            # return patches\n",
    "            for i in range(0, len(patches), batch_size):\n",
    "                batch = self.model.predict(patches[i: i + batch_size])\n",
    "                if i == 0:\n",
    "                    collect = batch\n",
    "                else:\n",
    "                    collect = np.append(collect, batch, axis=0)\n",
    "            \n",
    "            scale = self.scale\n",
    "            padded_size_scaled = tuple(np.multiply(p_shape[0:2], scale)) + (3,)\n",
    "            scaled_image_shape = tuple(np.multiply(input_image_array.shape[0:2], scale)) + (3,)\n",
    "            sr_img = stich_together(\n",
    "                collect,\n",
    "                padded_image_shape=padded_size_scaled,\n",
    "                target_shape=scaled_image_shape,\n",
    "                padding_size=padding_size * scale,\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            lr_img = process_array(input_image_array)\n",
    "            sr_img = self.model.predict(lr_img)[0]\n",
    "        \n",
    "        sr_img = process_output(sr_img)\n",
    "        return sr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.layers import concatenate, Input, Activation, Add, Conv2D, Lambda, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "class Generator:\n",
    "\n",
    "    def __init__(self,nbConvLayer,nbRDB,nbOutputCNN,nbOutputFilters,scalingFactor,patch_heigt,patch_width,cdim=3,kernel_size=3,upscaling='ups',init_extreme_val=0.05):\n",
    "        self.nbConvLayer = nbConvLayer\n",
    "        self.C = nbConvLayer\n",
    "        self.D = nbRDB\n",
    "        self.G = nbOutputCNN\n",
    "        self.G0 = nbOutputFilters\n",
    "        self.scale = scalingFactor\n",
    "        self.nbRDB = nbRDB\n",
    "        self.patch_heigt = patch_heigt\n",
    "        self.patch_width = patch_width\n",
    "        self.cdim = cdim\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.upscaling = upscaling\n",
    "        self.initializer = RandomUniform(\n",
    "            minval=-init_extreme_val, maxval=init_extreme_val, seed=None\n",
    "        )\n",
    "        self.model = self._build_rdn()\n",
    "        self.model._name = 'generator'\n",
    "        self.name = 'cnn-gan'\n",
    "        \n",
    "\n",
    "        \n",
    "    def _upsampling_block(self, input_layer):\n",
    "        \"\"\" Upsampling block for old weights. \"\"\"\n",
    "        \n",
    "        x = Conv2D(\n",
    "            self.cdim * self.scale ** 2,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            name='UPN3',\n",
    "            kernel_initializer=self.initializer,\n",
    "        )(input_layer)\n",
    "        return UpSampling2D(size=self.scale, name='UPsample')(x)\n",
    "    \n",
    "    def _pixel_shuffle(self, input_layer):\n",
    "        \"\"\" PixelShuffle implementation of the upscaling layer. \"\"\"\n",
    "        \n",
    "        x = Conv2D(\n",
    "            self.cdim * self.scale ** 2,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            name='UPN3',\n",
    "            kernel_initializer=self.initializer,\n",
    "        )(input_layer)\n",
    "        return Lambda(\n",
    "            lambda x: tf.nn.depth_to_space(x, block_size=self.scale, data_format='NHWC'),\n",
    "            name='PixelShuffle',\n",
    "        )(x)\n",
    "    \n",
    "    def _UPN(self, input_layer):\n",
    "        \"\"\" Upscaling layers. With old weights use _upsampling_block instead of _pixel_shuffle. \"\"\"\n",
    "        \n",
    "        x = Conv2D(\n",
    "            64,\n",
    "            kernel_size=5,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            name='UPN1',\n",
    "            kernel_initializer=self.initializer,\n",
    "        )(input_layer)\n",
    "        x = Activation('relu', name='UPN1_Relu')(x)\n",
    "        x = Conv2D(\n",
    "            32, kernel_size=3, padding='same', name='UPN2', kernel_initializer=self.initializer\n",
    "        )(x)\n",
    "        x = Activation('relu', name='UPN2_Relu')(x)\n",
    "        if self.upscaling == 'shuffle':\n",
    "            return self._pixel_shuffle(x)\n",
    "        elif self.upscaling == 'ups':\n",
    "            return self._upsampling_block(x)\n",
    "        else:\n",
    "            raise ValueError('Invalid choice of upscaling layer.')\n",
    "    \n",
    "    def _RDBs(self, input_layer):\n",
    "        \"\"\"RDBs blocks.\n",
    "\n",
    "        Args:\n",
    "            input_layer: input layer to the RDB blocks (e.g. the second convolutional layer F_0).\n",
    "\n",
    "        Returns:\n",
    "            concatenation of RDBs output feature maps with G0 feature maps.\n",
    "        \"\"\"\n",
    "        rdb_concat = list()\n",
    "        rdb_in = input_layer\n",
    "        for d in range(1, self.D + 1):\n",
    "            x = rdb_in\n",
    "            for c in range(1, self.C + 1):\n",
    "                F_dc = Conv2D(\n",
    "                    self.G,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    padding='same',\n",
    "                    kernel_initializer=self.initializer,\n",
    "                    name='F_%d_%d' % (d, c),\n",
    "                )(x)\n",
    "                F_dc = Activation('relu', name='F_%d_%d_Relu' % (d, c))(F_dc)\n",
    "                # concatenate input and output of ConvRelu block\n",
    "                # x = [input_layer,F_11(input_layer),F_12([input_layer,F_11(input_layer)]), F_13..]\n",
    "                x = concatenate([x, F_dc], axis=3, name='RDB_Concat_%d_%d' % (d, c))\n",
    "            # 1x1 convolution (Local Feature Fusion)\n",
    "            x = Conv2D(\n",
    "                self.G0, kernel_size=1, kernel_initializer=self.initializer, name='LFF_%d' % (d)\n",
    "            )(x)\n",
    "            # Local Residual Learning F_{i,LF} + F_{i-1}\n",
    "            rdb_in = Add(name='LRL_%d' % (d))([x, rdb_in])\n",
    "            rdb_concat.append(rdb_in)\n",
    "        \n",
    "        assert len(rdb_concat) == self.D\n",
    "        \n",
    "        return concatenate(rdb_concat, axis=3, name='LRLs_Concat')\n",
    "    \n",
    "    def _build_rdn(self):\n",
    "        LR_input = Input(shape=(self.patch_heigt, self.patch_width, 3), name='LR')\n",
    "        F_m1 = Conv2D(\n",
    "            self.G0,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding='same',\n",
    "            kernel_initializer=self.initializer,\n",
    "            name='F_m1',\n",
    "        )(LR_input)\n",
    "        F_0 = Conv2D(\n",
    "            self.G0,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding='same',\n",
    "            kernel_initializer=self.initializer,\n",
    "            name='F_0',\n",
    "        )(F_m1)\n",
    "        FD = self._RDBs(F_0)\n",
    "        # Global Feature Fusion\n",
    "        # 1x1 Conv of concat RDB layers -> G0 feature maps\n",
    "        GFF1 = Conv2D(\n",
    "            self.G0,\n",
    "            kernel_size=1,\n",
    "            padding='same',\n",
    "            kernel_initializer=self.initializer,\n",
    "            name='GFF_1',\n",
    "        )(FD)\n",
    "        GFF2 = Conv2D(\n",
    "            self.G0,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding='same',\n",
    "            kernel_initializer=self.initializer,\n",
    "            name='GFF_2',\n",
    "        )(GFF1)\n",
    "        # Global Residual Learning for Dense Features\n",
    "        FDF = Add(name='FDF')([GFF2, F_m1])\n",
    "        # Upscaling\n",
    "        FU = self._UPN(FDF)\n",
    "        # Compose SR image\n",
    "        SR = Conv2D(\n",
    "            self.cdim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding='same',\n",
    "            kernel_initializer=self.initializer,\n",
    "            name='SR',\n",
    "        )(FU)\n",
    "        \n",
    "        return Model(inputs=LR_input, outputs=SR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "\n",
    "\n",
    "class Cut_VGG19:\n",
    "    \"\"\"\n",
    "    Class object that fetches keras' VGG19 model trained on the imagenet dataset\n",
    "    and declares <layers_to_extract> as output layers. Used as feature extractor\n",
    "    for the perceptual loss function.\n",
    "\n",
    "    Args:\n",
    "        layers_to_extract: list of layers to be declared as output layers.\n",
    "        patch_size: integer, defines the size of the input (patch_size x patch_size).\n",
    "\n",
    "    Attributes:\n",
    "        loss_model: multi-output vgg architecture with <layers_to_extract> as output layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patch_heigt,patch_width, layers_to_extract):\n",
    "        self.patch_heigt = patch_heigt\n",
    "        self.patch_width = patch_width\n",
    "        self.input_shape = (patch_heigt, patch_width, 3)\n",
    "        self.layers_to_extract = layers_to_extract\n",
    "        \n",
    "        if len(self.layers_to_extract) > 0:\n",
    "            self._cut_vgg()\n",
    "        else:\n",
    "            raise ValueError('Invalid VGG instantiation: extracted layer must be > 0')\n",
    "    \n",
    "    def _cut_vgg(self):\n",
    "        \"\"\"\n",
    "        Loads pre-trained VGG, declares as output the intermediate\n",
    "        layers selected by self.layers_to_extract.\n",
    "        \"\"\"\n",
    "        \n",
    "        vgg = VGG19(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "        vgg.trainable = False\n",
    "        outputs = [vgg.layers[i].output for i in self.layers_to_extract]\n",
    "        self.model = Model([vgg.input], outputs)\n",
    "        self.model._name = 'feature_extractor'\n",
    "        self.name = 'vgg19'  # used in weights naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yoann\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "sample_interval = 1\n",
    "path_train = '../img/bug'\n",
    "path_test = 'data/test'\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = Discriminator(1080,1920)\n",
    "\n",
    "def discriminator_loss(real_output,fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "discriminator.model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "# discriminator.model.summary()\n",
    "# Generate random weights for the discriminator\n",
    "discriminator.model.save_weights('discriminator.h5')\n",
    "\n",
    "# Build the generator\n",
    "generator = Generator(3,3,4,4,4,1080//4,1920//4)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "generator.model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "# generator.model.summary()\n",
    "# Generate random weights for the generator\n",
    "generator.model.save_weights('generator.h5')\n",
    "\n",
    "# Build the VGG19 network\n",
    "vgg = Cut_VGG19(1080,1920, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "# Prepare the batches of training data\n",
    "train_paths = os.listdir(path_train)\n",
    "for i in range(len(train_paths)):\n",
    "    train_paths[i] = path_train + '/' + train_paths[i]\n",
    "\n",
    "nbBatches = len(train_paths) // batch_size\n",
    "\n",
    "# Separate the training data into batches of size batch_size\n",
    "batches_path_list = []\n",
    "for i in range(nbBatches):\n",
    "    batches_path_list.append(train_paths[i * batch_size:(i + 1) * batch_size])\n",
    "\n",
    "\n",
    "\n",
    "train_discriminator = True\n",
    "train_generator = False\n",
    "\n",
    "\n",
    "# Load the model weights\n",
    "if train_generator:\n",
    "    discriminator.model.load_weights('discriminator.h5')\n",
    "if train_discriminator:\n",
    "    generator.model.load_weights('generator.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    cross_validation_batches_index = np.random.randint(0, nbBatches, 1)\n",
    "    print('Cross validation batches index: ', cross_validation_batches_index)\n",
    "    for batch_i in tqdm(range(nbBatches)):\n",
    "        if batch_i == cross_validation_batches_index:\n",
    "            continue\n",
    "\n",
    "        # Load the data of the current batch\n",
    "        imgs_hr, imgs_lr = load_data(batches_path_list[batch_i], data_augmentation=0)\n",
    "\n",
    "        # ----------------------\n",
    "        #  Train Discriminator\n",
    "        # ----------------------\n",
    "        if train_discriminator:\n",
    "            # From low res. image generate high res. version\n",
    "            fake_hr = generator.model.predict(imgs_lr)\n",
    "            \n",
    "            \n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            d_loss_real = discriminator.model.train_on_batch(imgs_hr, valid)\n",
    "            d_loss_fake = discriminator.model.train_on_batch(fake_hr, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # ------------------\n",
    "        #  Train Generator\n",
    "        # ------------------\n",
    "        if train_generator:\n",
    "            # Train the generators\n",
    "            g_loss = generator.model.train_on_batch(imgs_lr, [imgs_hr, valid])\n",
    "        \n",
    "        # Plot the progress\n",
    "        # print(\n",
    "        #     \"\\n [Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %.2f%%] [G loss: %f] \\n\"\n",
    "        #     % (epoch, epochs, batch_i, nbBatches, d_loss,d_loss, g_loss)\n",
    "        # )\n",
    "    \n",
    "    # Calculate the cross validation loss after each epoch\n",
    "    imgs_hr, imgs_lr = load_data(batches_path_list[cross_validation_batches_index], data_augmentation=0)\n",
    "    fake_hr = generator.model.predict(imgs_lr)\n",
    "    cross_validation_loss_generator = generator.model.evaluate(imgs_lr, [imgs_hr, valid], verbose=0)\n",
    "    cross_validation_loss_discriminator = discriminator.model.evaluate(fake_hr, fake, verbose=0)\n",
    "\n",
    "    # Plot the progress\n",
    "    print(\n",
    "        \"\\n [Epoch %d/%d] [Cross validation loss generator: %f] [Cross validation loss discriminator: %f] \\n\"\n",
    "        % (epoch, epochs, cross_validation_loss_generator, cross_validation_loss_discriminator)\n",
    "    )\n",
    "\n",
    "\n",
    "# Save the model weights after training\n",
    "if train_discriminator:\n",
    "    discriminator.model.save_weights('discriminator.h5')\n",
    "if train_generator:\n",
    "    generator.model.save_weights('generator.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_paths):\n",
    "    imgs_hr, imgs_lr = load_data(batches_path_list[batch_i], data_augmentation=0)\n",
    "\n",
    "    # ----------------------\n",
    "    #  Train Discriminator\n",
    "    # ----------------------\n",
    "    # From low res. image generate high res. version\n",
    "    fake_hr = generator.model.predict(imgs_lr)\n",
    "    \n",
    "    \n",
    "    # Train the discriminators (original images = real / generated = Fake)\n",
    "    d_loss_real = discriminator.model.train_on_batch(imgs_hr, valid)\n",
    "    d_loss_fake = discriminator.model.train_on_batch(fake_hr, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ------------------\n",
    "    #  Train Generator\n",
    "    # ------------------\n",
    "    # Train the generators\n",
    "    g_loss = generator.model.train_on_batch(imgs_lr, [imgs_hr, valid])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
